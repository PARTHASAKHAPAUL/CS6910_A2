# -*- coding: utf-8 -*-
"""CS6910_A2_partB

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/cs6910-a2-partb-c21ec33d-05ce-4106-93c5-766b2b4029b0.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240408/auto/storage/goog4_request%26X-Goog-Date%3D20240408T180012Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Db65f5ed50eda6f881d465e0e735c58fe9073580408698feb9a902c5b6b4ae24310e750f351364838371e64703db5a3c98de3dc9c15b14b86d71e969dfb7dfc6b7443878244bcd678485442192eadbfe1cbf0cc5f443b1c63b8b7d6d1050db88c267e6961d8a4f25c50b49eeb14e83b04ba3e18483ed80aff7eddb32b4e8ea450dc83deb00461239b09ca45d30c038b4e22af852870fa8252ee97cda76b508a8d64a3003bab655145fa9e5d88ec8b2b911e803313682d98818638badbb1f5785979de0020482ca38a1612e5184e660b3de2c4c2306ecd0c1db84411a8e006ff112498febeb8f5b5de8c8656c2a91ef6eff44629aa38c1746879edb443f9916930

# CS6910_A2_PART:B
## Name: PARTHA SAKHA PAUL
## ROLL: MA23M016

# Importing all the essential libraries and the model ```resnet50``` for training with ```inaturalist dataset```
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import DataLoader
import torch.optim as optim
import numpy as np
from torchvision import models, transforms

# Load pre-trained ResNet50
model = models.resnet50(pretrained=True)

"""# And ```train_evaluate``` is same as in part A"""

# Method for training and evaluating the model
def train_and_evaluate(model, train_loader, validation_loader, n_epochs=10, lr=0.001, device=None):
    if device is None:
        # setting up CUDA if available, otherwise, CPU
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model.to(device)  # transferring the model to the selected device (GPU or CPU)

    # Initializing the loss function and optimizer
    loss_function = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)  # model.parameters() is a built-in method provided by PyTorch's nn.Module that collects all the learnable parameters of our model

    # Training loop
    for epoch in range(n_epochs):
        model.train()  # Setting model to training mode
        training_loss = 0.0
        # tracking number of correctly predicted instances
        correct_train = 0
        # tracking total number of instances
        total_train = 0

        # Iterate over the training data
        for images, labels in train_loader:
            # transferring images and labels to the current device (GPU or CPU)
            images, labels = images.to(device), labels.to(device)
            # clearing the gradients of all optimized variables
            optimizer.zero_grad()
            # Forward prop: computing predicted outputs by passing inputs to the model
            outputs = model(images)
            # calculating the loss between predicted and true labels
            loss = loss_function(outputs, labels)
            # Backward prop: computing gradient of the loss with respect to parameters
            loss.backward()
            # optimization step (parameter update)
            optimizer.step()
            # updating running training loss
            training_loss += loss.item() * images.size(0)
            # converting output probabilities to predicted class
            _, predicted_train = torch.max(outputs.data, 1)
            # updating total number of instances
            total_train += labels.size(0)
            # updating correctly predicted instances
            correct_train += (predicted_train == labels).sum().item()

        # average loss over an epoch
        training_loss /= len(train_loader.sampler)
        # train accuracy
        train_accuracy = correct_train / total_train

        # Evaluation phase
        model.eval()  # Setting model to evaluation mode
        # initializing the validation loss for the epoch
        validation_loss = 0.0
        # tracking number of correctly predicted instances
        correct_val = 0
        # tracking total number of instances
        total_val = 0
        # disable gradient calculation for validation, saving memory and computations
        with torch.no_grad():
            # iterating over the validation data loader
            for images, labels in validation_loader:
                # transferring images and labels to the current device (GPU or CPU)
                images, labels = images.to(device), labels.to(device)
                # Forward prop: computing predicted outputs by passing inputs to the model
                outputs = model(images)
                # calculating the loss between predicted and true labels
                loss = loss_function(outputs, labels)
                # updating running validation loss
                validation_loss += loss.item() * images.size(0)
                # converting output probabilities to predicted class
                _, predicted = torch.max(outputs.data, 1)
                # updating total number of instances
                total_val += labels.size(0)
                # updating correctly predicted instances
                correct_val += (predicted == labels).sum().item()

        # average validation loss over an epoch
        validation_loss /= len(validation_loader.sampler)
        # validation accuracy
        val_accuracy = correct_val / total_val

        # printing training and validation results
        print(f'Epoch {epoch+1}, Training Loss: {training_loss:.4f}, Training Accuracy: {train_accuracy*100:.2f}, \nValidation Loss: {validation_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}')
        wandb.log({"train_accuracy": train_accuracy * 100, "training_loss": training_loss})
        wandb.log({"val_accuracy": val_accuracy * 100, "val_loss": validation_loss})

import wandb
import numpy as np
from types import SimpleNamespace
import random

key = input('Enter your API:')
wandb.login(key=key)  #25c2257eaf6c22aa056893db14da4ee2bf0a531a

sweep_config = {
    'method': 'bayes',
    'name' : 'B2_kaggle',
    'metric': {
        'name': 'val_accuracy',
        'goal': 'maximize'
    },
    'parameters': {
        'freeze': {
            'values': [0.7,0.8,0.9]
        },
        'epochs': {
            'values': [5, 7]
        },
        'batch_size': {
            'values': [32, 64]
        },
    }
}

sweep_id = wandb.sweep(sweep = sweep_config, project="Deep_learning_A2_partB")

def main():
    # Initialize a new wandb run
    with wandb.init() as run:
        run_name = "-freeze-"+str(wandb.config.freeze)+"-epochs-"+str(wandb.config.epochs)+"-batch_size-"+str(wandb.config.batch_size)
        wandb.run.name=run_name


        def prepare_data_loaders(data_augment, batch_size=64):
            if data_augment:
                # With data augmentation
                train_transform = transforms.Compose([
                    transforms.RandomResizedCrop(224),
                    transforms.RandomHorizontalFlip(),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.450, 0.470, 0.490], std=[0.200, 0.220, 0.240]),
                ])
            else:
                # Without data augmentation
                train_transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                ])

            # Load datasets
            train_data = datasets.ImageFolder(root='/kaggle/input/inaturalist-12k/inaturalist_12K/train', transform=train_transform)
        #     validation_dataset = datasets.ImageFolder(root='/kaggle/input/inaturalist/inaturalist_12K/val', transform=val_transform)
        #     print(len(train_data))
            # print(train_data[9998])
            # Creating data indices for training and validation splits:
            dataset_size = len(train_data)  #9999
            indices = list(range(dataset_size)) #[0,1,...,9998]
            split = int(np.floor(0.2 * dataset_size))  #1999
        #     print(split)
            np.random.shuffle(indices)
            train_indices, val_indices = indices[split:], indices[:split]  # [1999,...,9998] , [0,...,1998]
            # print((val_indices))

            # Creating data samplers and loaders
            train_sampler = SubsetRandomSampler(train_indices)
            valid_sampler = SubsetRandomSampler(val_indices)
            # print(type(train_sampler))

            train_loader = DataLoader(train_data, batch_size=64, sampler=train_sampler)
            # print(len(train_loader))
            validation_loader = DataLoader(train_data, batch_size=64, sampler=valid_sampler)

            return train_loader, validation_loader


        # Load pre-trained ResNet50
        model = models.resnet50(pretrained=True)

        # In PyTorch, freezing a layer means setting its requires_grad attribute to False
        for param in model.parameters():
            param.requires_grad = False

        # Replace the last layer
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, 10)  # since 10 classes in the iNaturalist dataset

        # transformations
        data_transforms = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
        ])

        print(model)


        # Calculate total parameters and target
        total_params = sum(p.numel() for p in model.parameters())
        target_freeze = total_params * wandb.config.freeze

        # function to freeze parameters
        def freeze_parameters(model, target_freeze):
            frozen_params = 0
            for param in model.parameters():
                if frozen_params >= target_freeze:
                    break  # Stop freezing parameters when target is achieved
                param.requires_grad = False
                frozen_params += param.numel()  # tracking of how many parameters have been frozen

        # Freezing of parameters
        freeze_parameters(model, target_freeze)
        # print(model)

        # Replacing the last layer
        num_features = model.fc.in_features
        model.fc = nn.Linear(num_features, 10)  # since 10 classes in the iNaturalist dataset

        # preparing data loaders
        train_loader, validation_loader = prepare_data_loaders('false',batch_size = wandb.config.batch_size)
        # Model training
        train_and_evaluate(model, train_loader, validation_loader, n_epochs=wandb.config.epochs, lr=0.001)

wandb.agent(sweep_id, function=main, count=30)
wandb.finish()

