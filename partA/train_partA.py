# -*- coding: utf-8 -*-
"""CS6910_assignment2_partA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/cs6910-assignment2-parta-43b3b66c-bdaf-4a9a-9c74-90b814833cd2.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240408/auto/storage/goog4_request%26X-Goog-Date%3D20240408T175908Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3De93a887cd4f4de7e4552a129eababed0b948c4736c806b9c919e5108f8a9c419ef502655465915eb32814852a7117eb482939e452f732eab8762eac7ece2fce96a155b0e9b918552a7afba9d67c7c1b497e34a030f50e69dfee54aa74c3928e810831b803ae26c09f6555f5ac559775a4f413fee0c37d48ab5e5fe93d1f3180b35714156ae97fff3fc933b9a9fa3b40894d575fa89895c4183de85dc39da910df0c93757076039b6452d93287dda2b109e79517e952d0fbd099a226604342e50d788e490ee83cb9d69e51ebe0e1b10b00d184edebeea6202baa2ea6c9a4266b9a3e843b2b1e4407c816768342f4a1fa4049c13d49b0c5c76c668e4b6693490ae

# **NAME: PARTHA SAKHA PAUL**

# **ROLL: MA23M016**

    CS6910_assignment 2

**Importing all essential libraries**
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torchvision import datasets, transforms
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import DataLoader
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

"""**Image data transformation function for training and validation**"""

def prepare_data_loaders(data_augment, batch_size=64):
    if data_augment:
        # With data augmentation
        train_transform = transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.450, 0.470, 0.490], std=[0.200, 0.220, 0.240]),
        ])
    else:
        # Without data augmentation
        train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        ])


    # Load datasets
    train_data = datasets.ImageFolder(root='/kaggle/input/inaturalist-12k/inaturalist_12K/train', transform=train_transform)
#     validation_dataset = datasets.ImageFolder(root='/kaggle/input/inaturalist/inaturalist_12K/val', transform=val_transform)
#     print(len(train_data))
    # print(train_data[9998])
    # Creating data indices for training and validation splits:
    dataset_size = len(train_data)  #9999
    indices = list(range(dataset_size)) #[0,1,...,9998]
    split = int(np.floor(0.2 * dataset_size))  #1999
#     print(split)
    np.random.shuffle(indices)
    train_indices, val_indices = indices[split:], indices[:split]  # [1999,...,9998] , [0,...,1998]
    # print((val_indices))

    # Creating data samplers and loaders
    train_sampler = SubsetRandomSampler(train_indices)
    valid_sampler = SubsetRandomSampler(val_indices)
    # print(type(train_sampler))

    train_loader = DataLoader(train_data, batch_size=64, sampler=train_sampler)
    # print(len(train_loader))
    validation_loader = DataLoader(train_data, batch_size=64, sampler=valid_sampler)
    # print(*validation_loader)
    # Prepare data loaders
#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
#     validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, validation_loader

"""**Image size transformation function for training and test data**"""

# this is for data transformation for test dataset
def prepare_data_loaders_for_test(data_augment, batch_size=64):
    if data_augment:
        # With data augmentation
        transform_train = transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.450, 0.470, 0.490], std=[0.200, 0.220, 0.240]),
        ])
    else:
        # Without data augmentation
        transform_train = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
        ])


    # Load datasets
    train_data = datasets.ImageFolder(root='/kaggle/input/inaturalist-12k/inaturalist_12K/train', transform=transform_train)
    test_data = datasets.ImageFolder(root='/kaggle/input/inaturalist-12k/inaturalist_12K/val', transform=transform_train)  # applying same transformation on both dataset

    # Creating data loaders directly without splitting since val_data is already a separate dataset
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)  # for test dataset we dont need to shuffle but needed for visualization then it takes randomely

    return train_loader, test_loader

train_loader, test_loader = prepare_data_loaders_for_test('false')

"""**Activation functions dictionary and my_CNN class bulding**"""

activation_functions = {
            'relu': F.relu,
            'gelu': F.gelu,
            'silu': F.silu,
            'mish': F.mish
        }

# defining a class CNN that inherits from nn.Module which is PyTorch's base class for all neural network
class my_CNN(nn.Module):
    def __init__(self, num_classes=10, num_filters=[32, 64, 128, 256, 512], filter_sizes=[3, 3, 3, 3, 3], activation_fn='relu', num_neurons_dense = 256, dropout_rate = 0.5, use_batchnorm='no'):
        super(my_CNN, self).__init__()
        # Initializing class variables
        self.num_classes = num_classes
        self.num_filters = num_filters
        self.filter_sizes = filter_sizes
        self.activation_fn = activation_functions[activation_fn]
        self.num_neurons_dense = num_neurons_dense
        self.dropout_rate = dropout_rate
        # Convert yes/no to boolean
        self.use_batchnorm = True if use_batchnorm.lower() == 'yes' else False

        # Creating convolution layers using ModuleList
        self.conv_layers = nn.ModuleList()
        # List to hold batchnorm layers for conv
        self.batchnorm_layers_conv = nn.ModuleList()
        in_channels = 3  # since, input images are RGB
        for out_channels, kernel_size in zip(num_filters, filter_sizes):
            # a convolution layer with in-out channels and kernel size
            conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)
            self.conv_layers.append(conv_layer)
            if self.use_batchnorm:
                # Add a Batchnorm2d layer for each conv layer
                self.batchnorm_layers_conv.append(nn.BatchNorm2d(out_channels))
            # Update in_channels for the next layer
            in_channels = out_channels


        # Defining dropout before it's used
        self.dropout = nn.Dropout(dropout_rate)
        # find the size needed to flatten the conv layer outputs, initializing the dense layer accordingly
        self.init_flatten_size(input_shape=(3, 224, 224))  # input images are 224x224 RGB images
        # initializing the dense layer from flatten size
        self.dense = nn.Linear(self.flatten_size, num_neurons_dense)
        if self.use_batchnorm:
             # Batchnorm1d for the dense layer
            self.batchnorm_dense = nn.BatchNorm1d(num_neurons_dense)
        # output layer: maps from the dense layer to the number of classes.
        self.out = nn.Linear(num_neurons_dense, num_classes)


    def init_flatten_size(self, input_shape):
        # to disable gradient calculations for speed up this computation
        with torch.no_grad():
            # a dummy input tensor of the correct shape
            input_tensor = torch.zeros(1, *input_shape)
            # Forward prop through the conv layers to find output size
            output = self.forward_conv_layers(input_tensor)
            # total number of output features is the flat size needed for the dense layer input
            self.flatten_size = output.numel()

    # a function for sequentially passing input through all convolutional layers and applying activation and pooling
    def forward_conv_layers(self, x):
        # Convolution layer
        for i, conv in enumerate(self.conv_layers):
            x = conv(x)  # Convolution operation
            if self.use_batchnorm:
                # Apply batchnorm if enabled
                x = self.batchnorm_layers_conv[i](x)
            x = self.activation_fn(x)  # Apply the specified activation function
            x = F.max_pool2d(x, 2)  # max pooling with a kernel size of 2
            x = self.dropout(x)  # Apply dropout after pooling
        return x

    # forward method defines how the input tensor is transformed through the model
    def forward(self, x):
        # Passing input through the convolution blocks
        x = self.forward_conv_layers(x)
        # Flattening the output for the dense layer
        x = torch.flatten(x, 1)   # Flatten the output for the dense layer
        # Dense layer
        x = self.dense(x)
        if self.use_batchnorm:
            # Apply batchnorm if enabled to dense layer
            x = self.batchnorm_dense(x)
        x = self.activation_fn(x)
        # Applying dropout before the final layer
        x = self.dropout(x)
        # Output layer
        x = self.out(x)
        return x


    # Define a method for visualizing predictions made by the model.
    def visualize_predictions(self, data_loader, device, num_images=30):
        # Put the model in evaluation mode
        self.eval()
        # counter to keep track of how many images have been displayed
        images_occur = 0

        # Set the figure size for plotting
        plt.figure(figsize=(15, 30))

        # Use the torch.no_grad() to disable gradient calculation
        with torch.no_grad():
            # Iterate over batches of data from the data loader
            for images, labels in data_loader:
                # Move the images and labels to the specified device (CPU or GPU)
                images, labels = images.to(device), labels.to(device)
                # Forward pass: Compute the predicted outputs by passing images to the model
                outputs = self(images)
                # the predicted classes by finding the index with the maximum logit value for each image
                _, predicted = torch.max(outputs, 1)

                # Convert the images, labels, and predictions back to numpy arrays for visualization
                images = images.cpu().numpy()
                labels = labels.cpu().numpy()
                predicted = predicted.cpu().numpy()

                # Iterate over each image in the batch.
                for i in range(images.shape[0]):
                    # If the specified number of images (num_images) have already been displayed, exit the loop.
                    if images_occur >= num_images:
                        return
                    # Dynamically calculate the number of rows
                    num_rows = num_images // 3 + (num_images % 3 > 0)
                    # Create a subplot for each image
                    plt.subplot(num_rows, 3, images_occur + 1)
                    # Display the image
                    plt.imshow(np.transpose(images[i], (1, 2, 0)))
                    # Add a title to each subplot indicating the true and predicted labels
                    plt.title(f'True: {labels[i]}, Pred: {predicted[i]}')
                    plt.axis('off')
                    # Increment the counter
                    images_occur += 1

        # display the figure.
        plt.show()


    # Method for training and evaluating the model
    def train_and_evaluate(self, train_loader, validation_loader, n_epochs=10, lr=0.001, device=None):
        if device is None:
            # setting up CUDA if available, otherwise, using CPU
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.to(device)  # transferring the model to the selected device (GPU or CPU)

        # Initializing the loss function and optimizer
        loss_function = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.parameters(), lr=lr)  # self.parameters() is a built-in method provided by PyTorch's nn.Module that collects all the learnable parameters of our model

        # Training loop
        for epoch in range(n_epochs):
            self.train()  # Setting model to training mode
            training_loss = 0.0
            # tracking number of correctly predicted instances
            correct_train = 0
            # tracking total number of instances
            total_train = 0

            # Iterate over the training data
            for images, labels in train_loader:
                # transferring images and labels to the current device (GPU or CPU)
                images, labels = images.to(device), labels.to(device)
                # clearing the gradients of all optimized variables
                optimizer.zero_grad()
                # Forward prop: computing predicted outputs by passing inputs to the model
                outputs = self(images)
                # calculating the loss between predicted and true labels
                loss = loss_function(outputs, labels)
                # Backward prop: computing gradient of the loss with respect to parameters
                loss.backward()
                # optimization step (parameter update)
                optimizer.step()
                # updating running training loss
                training_loss += loss.item() * images.size(0)
                # converting output probabilities to predicted class
                _, predicted_train = torch.max(outputs.data, 1)
                # updating total number of instances
                total_train += labels.size(0)
                # updating correctly predicted instances
                correct_train += (predicted_train == labels).sum().item()

            # average loss over an epoch
            training_loss /= len(train_loader.sampler)
            # train accuracy
            train_accuracy = correct_train / total_train

            # Evaluation phase
            self.eval()  # Setting model to evaluation mode
            # initializing the validation loss for the epoch
            validation_loss = 0.0
            # tracking number of correctly predicted instances
            correct_val = 0
            # tracking total number of instances
            total_val = 0
            # disable gradient calculation for validation, saving memory and computations
            with torch.no_grad():
                # iterating over the validation data loader
                for images, labels in validation_loader:
                    # transferring images and labels to the current device (GPU or CPU)
                    images, labels = images.to(device), labels.to(device)
                    # Forward prop: computing predicted outputs by passing inputs to the model
                    outputs = self(images)
                    # calculating the loss between predicted and true labels
                    loss = loss_function(outputs, labels)
                    # updating running validation loss
                    validation_loss += loss.item() * images.size(0)
                    # converting output probabilities to predicted class
                    _, predicted = torch.max(outputs.data, 1)
                    # updating total number of instances
                    total_val += labels.size(0)
                    # updating correctly predicted instances
                    correct_val += (predicted == labels).sum().item()

            # average validation loss over an epoch
            validation_loss /= len(validation_loader.sampler)
            # validation accuracy
            val_accuracy = correct_val / total_val

            # printing training and validation results
#             print(f'Epoch {epoch+1}, Training Loss: {training_loss:.4f}, Training Accuracy: {train_accuracy*100:.2f}, \nValidation Loss: {validation_loss:.4f}, Validation Accuracy: {val_accuracy*100:.2f}')
            # printing training and testing results
            print(f'Epoch {epoch+1}, Training Loss: {training_loss:.4f}, Training Accuracy: {train_accuracy*100:.2f}, \nTest Loss: {validation_loss:.4f}, Test Accuracy: {val_accuracy*100:.2f}')
            # for validating the accuracy with validation data
#             wandb.log({"train_accuracy": train_accuracy * 100, "training_loss": training_loss})
#             wandb.log({"val_accuracy": val_accuracy * 100, "val_loss": validation_loss})
            # for testing the accuracy with test data
            wandb.log({"train_accuracy": train_accuracy * 100, "training_loss": training_loss})
            wandb.log({"test_accuracy": val_accuracy * 100, "test_loss": validation_loss})    # this val_accuracy actually gives test_accuracy when we pass the test data instead of validation data

        # After the last epoch, visualize predictions on test data
        self.visualize_predictions(test_loader, device)


# model with parameters
model = my_CNN(num_classes=10,
                  num_filters=[32, 64, 128, 256, 512],
                  filter_sizes=[7,9,7,9,9],
                  activation_fn='relu',
                  num_neurons_dense=512,
                  dropout_rate = 0.5,
                  use_batchnorm='yes')

print(model)

# model.train_and_evaluate(train_loader, validation_loader, n_epochs=10, lr=0.001)

"""**Importing WANDB for sweep**"""

import wandb
import numpy as np
from types import SimpleNamespace
import random

key = input('Enter your API:')
wandb.login(key=key)  #25c2257eaf6c22aa056893db14da4ee2bf0a531a

"""**Hyperparameter tuning for training data and validation data**"""

# this is for hyperparameter tuning in training dataset
sweep_config = {
    'method': 'bayes',
    'name' : 'cnn kaggle find best 3',
    'metric': {
        'name': 'val_accuracy',
        'goal': 'maximize'
    },
    'parameters': {
        'num_filters': {
            'values': [[128,128,64,64,32], [64, 64, 64, 64, 64], [256, 128, 64, 32, 16], [256,256,128,64,32],[256,128,64,32,16]]
        },
        'filter_sizes':{
            'values': [[7,7,5,3,3], [7,7,5,5,3], [7,7,7,5,3], [7,7,7,3,3], [5,5,5,5,5]]
        },
        'activation_fn': {
            'values': ['relu', 'gelu', 'silu', 'mish']
        },
        'dropout': {
            'values': [0.2, 0.3]
        },
        'batch_norm':{
            'values': ['false','true']
        },
        'data_augment': {
            'values': ['false','true']
        },
        'num_neurons_dense':{
            'values': [256,128]
        }
    }
}

sweep_id = wandb.sweep(sweep = sweep_config, project="Deep_learning_A2")

# this is for hyperparameter tuning in training dataset
def main():
    # Initialize a new wandb run
    with wandb.init() as run:
        run_name = "-ac-"+wandb.config.activation_fn+"-filters-"+str(wandb.config.num_filters)+"-filt_size-"+str(wandb.config.filter_sizes)+"-dropout-"+str(wandb.config.dropout)+"-batch_norm-"+wandb.config.batch_norm+"-data_aug-"+wandb.config.data_augment+"-num_neurons_dense-"+str(wandb.config.num_neurons_dense)
        wandb.run.name=run_name
        # Model object creation
        model = my_CNN(num_classes = 10,
                  num_filters = wandb.config.num_filters,
                  filter_sizes = wandb.config.filter_sizes,
                  activation_fn = wandb.config.activation_fn,
                  num_neurons_dense = wandb.config.num_neurons_dense,
                  dropout_rate = wandb.config.dropout,
                  use_batchnorm = wandb.config.batch_norm)

        data_augment = wandb.config.data_augment
        train_loader, validation_loader = prepare_data_loaders(data_augment)
        # Model training
        model.train_and_evaluate(train_loader, validation_loader, n_epochs=7, lr=0.001)

wandb.agent(sweep_id, function=main, count=20)
wandb.finish()

"""**Checking accuracy for training data and test data by best configuration**"""

# Best configuration from train dataset after running approx 150 count: -ac-mish-filters-[64, 64, 64, 64, 64]-filt_size-[7, 5, 3, 3, 5]-dropout-0-batch_norm-false-data_aug-false-num_neurons_dense-256

# this sweep is for checking accuracy in test dataset
sweep_config = {
    'method': 'bayes',
    'name' : 'cnn kaggle test print',
    'metric': {
        'name': 'te_accuracy',
        'goal': 'maximize'
    },
    'parameters': {
        'num_filters': {
            'values': [[64, 64, 64, 64, 64]]
        },
        'filter_sizes':{
            'values': [[7,5,3,3,5]]
        },
        'activation_fn': {
            'values': ['mish']
        },
        'dropout': {
            'values': [0]
        },
        'batch_norm':{
            'values': ['false']
        },
        'data_augment': {
            'values': ['false']
        },
        'num_neurons_dense':{
            'values': [256]
        }
    }
}

sweep_id = wandb.sweep(sweep = sweep_config, project="Deep_learning_A2")

# this is for checking accuracy in test dataset
def main():
    # Initialize a new wandb run
    with wandb.init() as run:
        run_name = "-ac-"+wandb.config.activation_fn+"-filters-"+str(wandb.config.num_filters)+"-filt_size-"+str(wandb.config.filter_sizes)+"-dropout-"+str(wandb.config.dropout)+"-batch_norm-"+wandb.config.batch_norm+"-data_aug-"+wandb.config.data_augment+"-num_neurons_dense-"+str(wandb.config.num_neurons_dense)
        wandb.run.name=run_name
        # Model object creation
        model = my_CNN(num_classes = 10,
                  num_filters = wandb.config.num_filters,
                  filter_sizes = wandb.config.filter_sizes,
                  activation_fn = wandb.config.activation_fn,
                  num_neurons_dense = wandb.config.num_neurons_dense,
                  dropout_rate = wandb.config.dropout,
                  use_batchnorm = wandb.config.batch_norm)

        data_augment = wandb.config.data_augment
        train_loader, test_loader = prepare_data_loaders_for_test(data_augment)
        # Model training
        model.train_and_evaluate(train_loader, test_loader, n_epochs=8, lr=0.001)

wandb.agent(sweep_id, function=main, count=1)
wandb.finish()

