{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NAME: PARTHA SAKHA PAUL**\n",
        "\n",
        "**ROLL: MA23M016**\n",
        "\n",
        "    CS6910_assignment 2\n"
      ],
      "metadata": {
        "id": "raGO2MwAaOyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fI-FF-_KPBF",
        "outputId": "e7aa91e8-5358-4c87-e94b-8eadbfbcd171"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdtm9tedNYef",
        "outputId": "21d7ea07-eb22-4056-bc85-62fe21063452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'\t   ma23m016_lab7.mp4\t     ma23m016_video_lab3.mp4\n",
            "'Copy of template.gdoc'    ma23m016_lab8.mp4\t     ma23m016_video_lab4.mp4\n",
            " ma23m016_lab10.mp4\t   ma23m016_lab9.mp4\t     nature_12K\n",
            " ma23m016_lab5_video.mp4   ma23m016-video-hw1.webm   nature_12K.zip\n",
            " ma23m016_lab6.mp4\t   ma23m016_video_hw2.mp4    Screenshot_20240207-220942_GPay.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/My Drive/nature_12K.zip' -d '/content/drive/My Drive/nature_12K'"
      ],
      "metadata": {
        "id": "-ZcoR6cHLKtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "data_dir = '/content/drive/My Drive/nature_12K'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=transform)"
      ],
      "metadata": {
        "id": "SmPJexuVZl3L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fIdHjk5l8HgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "data_dir = '/content/drive/My Drive/nature_12K/inaturalist_12K'\n",
        "\n",
        "# Defining transforms for the training, validation, and testing sets\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the datasets with ImageFolder\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=transform)\n",
        "print(len(train_data))\n",
        "# print(train_data[9998])\n",
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(train_data)  #9999\n",
        "indices = list(range(dataset_size)) #[0,1,...,9998]\n",
        "split = int(np.floor(0.2 * dataset_size))  #1999\n",
        "print(split)\n",
        "np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]  # [1999,...,9998] , [0,...,1998]\n",
        "# print((val_indices))\n",
        "\n",
        "# Creating data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "# print(type(train_sampler))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, sampler=train_sampler)\n",
        "# print(len(train_loader))\n",
        "validation_loader = torch.utils.data.DataLoader(train_data, batch_size=64, sampler=valid_sampler)\n",
        "# print(*validation_loader)"
      ],
      "metadata": {
        "id": "P1k_rjcaqYV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d38b9c-0f10-4d2b-ff6c-2009c1658895"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9999\n",
            "1999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a class my_CNN that inherits from nn.Module which is PyTorch's base class for all neural network\n",
        "class my_CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, num_filters=[32, 64, 128, 256, 512], filter_sizes=[3, 3, 3, 3, 3], activation_fn=F.relu, num_neurons_dense=512):\n",
        "        super(my_CNN, self).__init__()\n",
        "        # Initializing class variables\n",
        "        self.num_classes = num_classes\n",
        "        self.num_filters = num_filters\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self.activation_fn = activation_fn\n",
        "        self.num_neurons_dense = num_neurons_dense\n",
        "\n",
        "        # Creating convolution layers using ModuleList\n",
        "        self.conv_layers = nn.ModuleList()\n",
        "        in_channels = 3  # since, input images are RGB\n",
        "        for out_channels, kernel_size in zip(num_filters, filter_sizes):\n",
        "            # a convolutional layer with in-out channels and kernel size\n",
        "            conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, padding=0)\n",
        "            self.conv_layers.append(conv_layer)\n",
        "            # Update in_channels for the next layer\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # Placeholder for dense layer will be initialized after calculating the flatten size\n",
        "        self.dense = None\n",
        "        # output layer: maps from the dense layer to the number of classes.\n",
        "        self.out = nn.Linear(num_neurons_dense, num_classes)\n",
        "        # find the size needed to flatten the conv layer outputs, initializing the dense layer accordingly\n",
        "        self.init_flatten_size(input_shape=(3, 224, 224))  # input images are 224x224 RGB images\n",
        "        # initializing the dense layer from flatten size\n",
        "        self.dense = nn.Linear(self.flatten_size, num_neurons_dense)\n",
        "\n",
        "\n",
        "    def init_flatten_size(self, input_shape):\n",
        "        # to disable gradient calculations for speed up this computation\n",
        "        with torch.no_grad():\n",
        "            # a dummy input tensor of the correct shape\n",
        "            input_tensor = torch.zeros(1, *input_shape)\n",
        "            # Forward propagate through the conv layers to find output size\n",
        "            output = self.forward_conv_layers(input_tensor)\n",
        "            # total number of output features is the flat size needed for the dense layer input\n",
        "            self.flatten_size = output.numel()\n",
        "\n",
        "    # a function for sequentially passing input through all convolutional layers and applying activation and pooling\n",
        "    def forward_conv_layers(self, x):\n",
        "        for conv in self.conv_layers:\n",
        "            x = conv(x)\n",
        "            x = self.activation_fn(x)\n",
        "            x = F.max_pool2d(x, 2)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolution blocks\n",
        "        for conv in self.conv_layers:\n",
        "            x = conv(x)\n",
        "            x = self.activation_fn(x)\n",
        "            x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # Flattening the output for the dense layer\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Dense layer\n",
        "        x = self.dense(x)\n",
        "        x = self.activation_fn(x)\n",
        "        # Output layer\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "# model with parameters\n",
        "model = my_CNN(num_classes=10,\n",
        "                  num_filters=[32, 64, 128, 256, 512],\n",
        "                  filter_sizes=[3, 3, 3, 3, 3],\n",
        "                  activation_fn=F.relu,\n",
        "                  num_neurons_dense=512)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEdslUMdH-uH",
        "outputId": "6b244dd3-9f7d-4eaf-9ce0-7d4231b91b96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_CNN(\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "  )\n",
            "  (out): Linear(in_features=512, out_features=10, bias=True)\n",
            "  (dense): Linear(in_features=12800, out_features=512, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GVAGOb1KAwUK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}