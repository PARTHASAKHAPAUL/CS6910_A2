{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8000604,"sourceType":"datasetVersion","datasetId":4711316}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **NAME: PARTHA SAKHA PAUL**\n\n# **ROLL: MA23M016**\n\n    CS6910_assignment 2\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision import datasets, transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.optim as optim\nimport numpy as np\n\n# data_dir = '/content/drive/My Drive/nature_12K'\ndata_dir = '/kaggle/input/inaturalist/inaturalist_12K/train'\n\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\ndataset = datasets.ImageFolder(root=data_dir, transform=transform)\n\n# data_dir = '/content/drive/My Drive/nature_12K/inaturalist_12K'\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-04T10:35:21.797461Z","iopub.execute_input":"2024-04-04T10:35:21.797785Z","iopub.status.idle":"2024-04-04T10:35:32.107517Z","shell.execute_reply.started":"2024-04-04T10:35:21.797759Z","shell.execute_reply":"2024-04-04T10:35:32.106489Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining transforms for the training, validation, and testing sets\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])\n\n# Load the datasets with ImageFolder\ntrain_data = datasets.ImageFolder(data_dir, transform=transform)\nprint(len(train_data))\n# print(train_data[9998])\n# Creating data indices for training and validation splits:\ndataset_size = len(train_data)  #9999\nindices = list(range(dataset_size)) #[0,1,...,9998]\nsplit = int(np.floor(0.2 * dataset_size))  #1999\nprint(split)\nnp.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]  # [1999,...,9998] , [0,...,1998]\n# print((val_indices))\n\n# Creating data samplers and loaders\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n# print(type(train_sampler))\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=64, sampler=train_sampler)\n# print(len(train_loader))\nvalidation_loader = torch.utils.data.DataLoader(train_data, batch_size=64, sampler=valid_sampler)\n# print(*validation_loader)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T10:35:35.486181Z","iopub.execute_input":"2024-04-04T10:35:35.486774Z","iopub.status.idle":"2024-04-04T10:35:36.802083Z","shell.execute_reply.started":"2024-04-04T10:35:35.486740Z","shell.execute_reply":"2024-04-04T10:35:36.801164Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"9999\n1999\n","output_type":"stream"}]},{"cell_type":"code","source":"activation_functions = {\n            'relu': F.relu,\n            'gelu': F.gelu,\n            'silu': F.silu,\n            'mish': F.mish\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining a class CNN that inherits from nn.Module which is PyTorch's base class for all neural network\nclass my_CNN(nn.Module):\n    def __init__(self, num_classes=10, num_filters=[32, 64, 128, 256, 512], filter_sizes=[3, 3, 3, 3, 3], activation_fn='relu', num_neurons_dense=512):\n        super(my_CNN, self).__init__()\n        # Initializing class variables\n        self.num_classes = num_classes\n        self.num_filters = num_filters\n        self.filter_sizes = filter_sizes\n        self.activation_fn = activation_functions[activation_fn]\n        self.num_neurons_dense = num_neurons_dense\n\n        # Creating convolution layers using ModuleList\n        self.conv_layers = nn.ModuleList()\n        in_channels = 3  # since, input images are RGB\n        for out_channels, kernel_size in zip(num_filters, filter_sizes):\n            # a convolution layer with in-out channels and kernel size\n            conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n            self.conv_layers.append(conv_layer)\n            # Update in_channels for the next layer\n            in_channels = out_channels\n\n            \n        # find the size needed to flatten the conv layer outputs, initializing the dense layer accordingly\n        self.init_flatten_size(input_shape=(3, 224, 224))  # input images are 224x224 RGB images\n        # initializing the dense layer from flatten size\n        self.dense = nn.Linear(self.flatten_size, num_neurons_dense)\n        # output layer: maps from the dense layer to the number of classes.\n        self.out = nn.Linear(num_neurons_dense, num_classes)\n\n\n    def init_flatten_size(self, input_shape):\n        # to disable gradient calculations for speed up this computation\n        with torch.no_grad():\n            # a dummy input tensor of the correct shape\n            input_tensor = torch.zeros(1, *input_shape)\n            # Forward prop through the conv layers to find output size\n            output = self.forward_conv_layers(input_tensor)\n            # total number of output features is the flat size needed for the dense layer input\n            self.flatten_size = output.numel()\n\n    # a function for sequentially passing input through all convolutional layers and applying activation and pooling\n    def forward_conv_layers(self, x):\n        # Convolution layer\n        for conv in self.conv_layers:\n            x = conv(x)  # Convolution operation\n            x = self.activation_fn(x)  # Apply the specified activation function\n            x = F.max_pool2d(x, 2)  # Apply max pooling with a kernel size of 2\n        return x\n\n    # forward method defines how the input tensor is transformed through the model\n    def forward(self, x):\n        # Passing input through the convolution blocks\n        x = self.forward_conv_layers(x)\n        # Flattening the output for the dense layer\n        x = torch.flatten(x, 1)   # Flatten the output for the dense layer\n        # Dense layer\n        x = self.dense(x)\n        x = self.activation_fn(x)\n        # Output layer\n        x = self.out(x)\n        return x\n\n    # Method for training and evaluating the model\n    def train_and_evaluate(self, train_loader, validation_loader, n_epochs=10, lr=0.001, device=None):\n        if device is None:\n            # setting up CUDA if available, otherwise, using CPU\n            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.to(device)  # transferring the model to the selected device (GPU or CPU)\n\n        # Initializing the loss function and optimizer\n        loss_function = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.parameters(), lr=lr)  # self.parameters() is a built-in method provided by PyTorch's nn.Module that collects all the learnable parameters of our model\n\n        # Training loop\n        for epoch in range(n_epochs):\n            self.train()  # Setting model to training mode\n            training_loss = 0.0\n\n            # Iterate over the training data\n            for images, labels in train_loader:\n                # transferring images and labels to the current device (GPU or CPU)\n                images, labels = images.to(device), labels.to(device)\n                # clearing the gradients of all optimized variables\n                optimizer.zero_grad()\n                # Forward prop: computing predicted outputs by passing inputs to the model\n                outputs = self(images)\n                # calculating the loss between predicted and true labels\n                loss = loss_function(outputs, labels)\n                # Backward prop: computing gradient of the loss with respect to parameters\n                loss.backward()\n                # optimization step (parameter update)\n                optimizer.step()\n                # updating running training loss\n                training_loss += loss.item() * images.size(0)\n            \n            # average loss over an epoch\n            training_loss /= len(train_loader.sampler)\n\n            # Evaluation phase\n            self.eval()  # Setting model to evaluation mode\n            # initializing the validation loss for the epoch\n            validation_loss = 0.0\n            # tracking number of correctly predicted instances\n            correct = 0\n            # tracking total number of instances\n            total = 0\n            # disable gradient calculation for validation, saving memory and computations\n            with torch.no_grad():\n                # iterating over the validation data loader\n                for images, labels in validation_loader:\n                    # transferring images and labels to the current device (GPU or CPU)\n                    images, labels = images.to(device), labels.to(device)\n                    # Forward prop: computing predicted outputs by passing inputs to the model\n                    outputs = self(images)\n                    # calculating the loss between predicted and true labels\n                    loss = loss_function(outputs, labels)\n                    # updating running validation loss\n                    validation_loss += loss.item() * images.size(0)\n                    # converting output probabilities to predicted class\n                    _, predicted = torch.max(outputs.data, 1)\n                    # updating total number of instances\n                    total += labels.size(0)\n                    # updating correctly predicted instances\n                    correct += (predicted == labels).sum().item()\n\n            # average validation loss over an epoch\n            validation_loss /= len(validation_loader.sampler)\n            # validation accuracy\n            val_accuracy = correct / total\n\n            # printing training and validation results\n            print(f'Epoch {epoch+1}, Training Loss: {training_loss:.4f}, Validation Loss: {validation_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n            wandb.log({\"val_accuracy\": val_accuracy * 100, \"loss\": validation_loss})\n\n\n# model with parameters\nmodel = my_CNN(num_classes=10,\n                  num_filters=[32, 64, 128, 256, 512],\n                  filter_sizes=[3, 3, 3, 3, 3],\n                  activation_fn='relu',\n                  num_neurons_dense=512)\n\nprint(model)\n\n# model.train_and_evaluate(train_loader, validation_loader, n_epochs=10, lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T10:35:37.857540Z","iopub.execute_input":"2024-04-04T10:35:37.857905Z","iopub.status.idle":"2024-04-04T10:35:38.139859Z","shell.execute_reply.started":"2024-04-04T10:35:37.857874Z","shell.execute_reply":"2024-04-04T10:35:38.138914Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"my_CNN(\n  (conv_layers): ModuleList(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  )\n  (dense): Linear(in_features=25088, out_features=512, bias=True)\n  (out): Linear(in_features=512, out_features=10, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport numpy as np\nfrom types import SimpleNamespace\nimport random\n\nkey = input('Enter your API:')\nwandb.login(key=key)  #25c2257eaf6c22aa056893db14da4ee2bf0a531a","metadata":{"execution":{"iopub.status.busy":"2024-04-04T10:35:43.916229Z","iopub.execute_input":"2024-04-04T10:35:43.916590Z","iopub.status.idle":"2024-04-04T10:35:51.908358Z","shell.execute_reply.started":"2024-04-04T10:35:43.916560Z","shell.execute_reply":"2024-04-04T10:35:51.907460Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your API: 25c2257eaf6c22aa056893db14da4ee2bf0a531a\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',  # can be grid, random\n    'name' : 'cnn kaggle new',\n    'metric': {\n        'name': 'val_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'num_filters': {\n            'values': [[32, 64, 128, 256, 512], [64, 64, 64, 64, 64], [512, 256, 128, 64, 32]] \n        },\n        'filter_sizes':{\n            'values': [[3,5,5,7,7], [7,5,3,3,5], [3,5,7,9,5]]\n        },\n        'activation_fn': {\n            'values': ['relu', 'gelu', 'silu', 'mish']\n        },\n#         'dropout': {\n#             'values': [0.2, 0.3]\n#         },\n#         'batch_norm':{\n#             'values': ['true','false']\n#         },\n#         'data_augment': {\n#             'values': ['true','false']\n#         },\n        'num_neurons_dense':{\n            'values': [64, 128, 256, 512]\n        }\n    }\n}\n\nsweep_id = wandb.sweep(sweep = sweep_config, project=\"Deep_learning_A2\")","metadata":{"execution":{"iopub.status.busy":"2024-04-04T10:37:55.685126Z","iopub.execute_input":"2024-04-04T10:37:55.685695Z","iopub.status.idle":"2024-04-04T10:37:56.097620Z","shell.execute_reply.started":"2024-04-04T10:37:55.685665Z","shell.execute_reply":"2024-04-04T10:37:56.096726Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Create sweep with ID: u2oec5rw\nSweep URL: https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw\n","output_type":"stream"}]},{"cell_type":"code","source":"def main():\n    # Initialize a new wandb run\n    with wandb.init() as run:\n        run_name = \"-ac-\"+wandb.config.activation_fn+\"-filters-\"+str(wandb.config.num_filters)+\"-filt_size-\"+str(wandb.config.filter_sizes)+\"-num_neurons_dense-\"+str(wandb.config.num_neurons_dense)\n        wandb.run.name=run_name\n        # Model training\n        model = my_CNN(num_classes=10,\n                  num_filters=wandb.config.num_filters,\n                  filter_sizes=wandb.config.filter_sizes,\n                  activation_fn=wandb.config.activation_fn,\n                  num_neurons_dense=wandb.config.num_neurons_dense)\n        model.train_and_evaluate(train_loader, validation_loader, n_epochs=10, lr=0.001)        \n\nwandb.agent(sweep_id, function=main, count=5)  # to run 5 experiments\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T10:38:00.571299Z","iopub.execute_input":"2024-04-04T10:38:00.571945Z","iopub.status.idle":"2024-04-04T11:22:39.679382Z","shell.execute_reply.started":"2024-04-04T10:38:00.571911Z","shell.execute_reply":"2024-04-04T11:22:39.678306Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 58zih8xy with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 5, 5, 7, 7]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: [64, 64, 64, 64, 64]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons_dense: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mparthasakhapaul\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240404_103805-58zih8xy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/58zih8xy' target=\"_blank\">feasible-sweep-1</a></strong> to <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/58zih8xy' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/58zih8xy</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1, Training Loss: 2.2342, Validation Loss: 2.2017, Validation Accuracy: 0.1846\nEpoch 2, Training Loss: 2.0997, Validation Loss: 2.1229, Validation Accuracy: 0.2391\nEpoch 3, Training Loss: 2.0202, Validation Loss: 1.9908, Validation Accuracy: 0.2856\nEpoch 4, Training Loss: 1.9657, Validation Loss: 2.0447, Validation Accuracy: 0.2886\nEpoch 5, Training Loss: 1.9161, Validation Loss: 1.9780, Validation Accuracy: 0.2896\nEpoch 6, Training Loss: 1.8290, Validation Loss: 1.9720, Validation Accuracy: 0.2846\nEpoch 7, Training Loss: 1.7512, Validation Loss: 1.9641, Validation Accuracy: 0.3117\nEpoch 8, Training Loss: 1.6203, Validation Loss: 2.0138, Validation Accuracy: 0.3142\nEpoch 9, Training Loss: 1.4941, Validation Loss: 2.1148, Validation Accuracy: 0.2961\nEpoch 10, Training Loss: 1.2559, Validation Loss: 2.2173, Validation Accuracy: 0.3207\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▅▂▃▁▁▁▂▅█</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▆▆██▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>2.21734</td></tr><tr><td>val_accuracy</td><td>32.06603</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">feasible-sweep-1</strong> at: <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/58zih8xy' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/58zih8xy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_103805-58zih8xy/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o704gdqv with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [7, 5, 3, 3, 5]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: [64, 64, 64, 64, 64]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons_dense: 64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240404_105942-o704gdqv</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/o704gdqv' target=\"_blank\">comic-sweep-2</a></strong> to <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/o704gdqv' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/o704gdqv</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1, Training Loss: 2.2674, Validation Loss: 2.2142, Validation Accuracy: 0.1671\nEpoch 2, Training Loss: 2.1530, Validation Loss: 2.1227, Validation Accuracy: 0.2166\nEpoch 3, Training Loss: 2.0611, Validation Loss: 2.0215, Validation Accuracy: 0.2731\nEpoch 4, Training Loss: 1.9738, Validation Loss: 2.0093, Validation Accuracy: 0.2831\nEpoch 5, Training Loss: 1.9186, Validation Loss: 1.9640, Validation Accuracy: 0.2981\nEpoch 6, Training Loss: 1.8437, Validation Loss: 1.9987, Validation Accuracy: 0.2941\nEpoch 7, Training Loss: 1.7484, Validation Loss: 1.9380, Validation Accuracy: 0.3162\nEpoch 8, Training Loss: 1.6760, Validation Loss: 2.0956, Validation Accuracy: 0.2981\nEpoch 9, Training Loss: 1.5545, Validation Loss: 1.9497, Validation Accuracy: 0.3497\nEpoch 10, Training Loss: 1.4150, Validation Loss: 2.0706, Validation Accuracy: 0.3177\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▆▃▃▂▃▁▅▁▄</td></tr><tr><td>val_accuracy</td><td>▁▃▅▅▆▆▇▆█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>2.0706</td></tr><tr><td>val_accuracy</td><td>31.76588</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">comic-sweep-2</strong> at: <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/o704gdqv' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/o704gdqv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_105942-o704gdqv/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xkxl5qk6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 5, 7, 9, 11]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: [64, 64, 64, 64, 64]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons_dense: 64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240404_112018-xkxl5qk6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/xkxl5qk6' target=\"_blank\">sweet-sweep-3</a></strong> to <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/xkxl5qk6' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/xkxl5qk6</a>"},"metadata":{}},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_34/2820147712.py\", line 7, in main\n    model = my_CNN(num_classes=10,\n  File \"/tmp/ipykernel_34/2571271743.py\", line 32, in __init__\n    self.init_flatten_size(input_shape=(3, 224, 224))  # input images are 224x224 RGB images\n  File \"/tmp/ipykernel_34/2571271743.py\", line 45, in init_flatten_size\n    output = self.forward_conv_layers(input_tensor)\n  File \"/tmp/ipykernel_34/2571271743.py\", line 55, in forward_conv_layers\n    x = F.max_pool2d(x, 2)  # Apply max pooling with a kernel size of 2\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py\", line 488, in fn\n    return if_false(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 791, in _max_pool2d\n    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\nRuntimeError: Given input size: (64x1x1). Calculated output size: (64x0x0). Output size is too small\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sweet-sweep-3</strong> at: <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/xkxl5qk6' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/xkxl5qk6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_112018-xkxl5qk6/logs</code>"},"metadata":{}},{"name":"stderr","text":"Run xkxl5qk6 errored:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n    self._function()\n  File \"/tmp/ipykernel_34/2820147712.py\", line 7, in main\n    model = my_CNN(num_classes=10,\n  File \"/tmp/ipykernel_34/2571271743.py\", line 32, in __init__\n    self.init_flatten_size(input_shape=(3, 224, 224))  # input images are 224x224 RGB images\n  File \"/tmp/ipykernel_34/2571271743.py\", line 45, in init_flatten_size\n    output = self.forward_conv_layers(input_tensor)\n  File \"/tmp/ipykernel_34/2571271743.py\", line 55, in forward_conv_layers\n    x = F.max_pool2d(x, 2)  # Apply max pooling with a kernel size of 2\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py\", line 488, in fn\n    return if_false(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 791, in _max_pool2d\n    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\nRuntimeError: Given input size: (64x1x1). Calculated output size: (64x0x0). Output size is too small\n\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xkxl5qk6 errored:\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_34/2820147712.py\", line 7, in main\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = my_CNN(num_classes=10,\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_34/2571271743.py\", line 32, in __init__\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.init_flatten_size(input_shape=(3, 224, 224))  # input images are 224x224 RGB images\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_34/2571271743.py\", line 45, in init_flatten_size\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = self.forward_conv_layers(input_tensor)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_34/2571271743.py\", line 55, in forward_conv_layers\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = F.max_pool2d(x, 2)  # Apply max pooling with a kernel size of 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py\", line 488, in fn\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return if_false(*args, **kwargs)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py\", line 791, in _max_pool2d\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m RuntimeError: Given input size: (64x1x1). Calculated output size: (64x0x0). Output size is too small\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: g0drjwb7 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_fn: silu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_sizes: [3, 5, 5, 7, 7]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: [64, 64, 64, 64, 64]\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons_dense: 512\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240404_112121-g0drjwb7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/g0drjwb7' target=\"_blank\">efficient-sweep-4</a></strong> to <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/sweeps/u2oec5rw</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/g0drjwb7' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/g0drjwb7</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">efficient-sweep-4</strong> at: <a href='https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/g0drjwb7' target=\"_blank\">https://wandb.ai/parthasakhapaul/Deep_learning_A2/runs/g0drjwb7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_112121-g0drjwb7/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}